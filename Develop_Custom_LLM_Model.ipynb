{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEf3KCbAcXpX"
      },
      "source": [
        "# Building an LLM from scratch\n",
        "## Code Description\n",
        "This code is implementing a text generation model using PyTorch, a popular machine learning library. The model is trained on a large corpus of text and learns to predict the next word in a sequence given the previous words. This type of model can be used for a variety of natural language processing tasks, such as text completion, translation, and more.\n",
        "\n",
        "Let's break down the code into its main components:\n",
        "\n",
        "## Loading dataset\n",
        "The code also includes a function to load abstracts from Semantic Scholar, a free, AI-powered research tool for scientific literature. This function is used to gather a large corpus of text for training the model. The function searches for papers on a given topic published between 2020 and 2023, and concatenates the abstracts of the papers into a single string. The function also maintains a list of individual abstracts. The function stops and returns the text and the list of abstracts once it has processed a specified number of papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YShQspifcnY-",
        "outputId": "8633c010-70c9-440a-ebd9-2c3d3e11f764"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: semanticscholar in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (0.8.0)\n",
            "Requirement already satisfied: tenacity in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (from semanticscholar) (8.2.3)\n",
            "Requirement already satisfied: httpx in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (from semanticscholar) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (from semanticscholar) (1.6.0)\n",
            "Requirement already satisfied: anyio in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (from httpx->semanticscholar) (4.3.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (from httpx->semanticscholar) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (from httpx->semanticscholar) (1.0.5)\n",
            "Requirement already satisfied: idna in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (from httpx->semanticscholar) (3.6)\n",
            "Requirement already satisfied: sniffio in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (from httpx->semanticscholar) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (from httpcore==1.*->httpx->semanticscholar) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install semanticscholar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZHdA6tp3HQOX"
      },
      "outputs": [],
      "source": [
        "from semanticscholar import SemanticScholar\n",
        "from functools import lru_cache\n",
        "\n",
        "MAX_PAPER = 600\n",
        "\n",
        "@lru_cache\n",
        "def load_abstracts(topic=\"generative ai\", number_paper=MAX_PAPER):\n",
        "    sch = SemanticScholar()\n",
        "    papers = sch.search_paper(query=topic, year=\"2018-2023\")\n",
        "    big_text = \"\"\n",
        "    abstract_list = []\n",
        "    for i, paper in enumerate(papers):\n",
        "        abstract = paper['abstract']\n",
        "        if abstract != None:\n",
        "            big_text += f\"\\n<START-ABSTRACT {i}>: \\n{abstract}\\n</END-ABSTRACT {i}\\n\"\n",
        "            abstract_list.append(abstract)\n",
        "        if i > number_paper:\n",
        "            return big_text, abstract_list\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLJjWHSlc8PD"
      },
      "source": [
        "## Importing Libraries\n",
        "\n",
        "The first part of the code is importing all the necessary libraries. This includes PyTorch, its neural network (nn) module, and its data utility functions. It also imports a tokenizer from torchtext, a library for text processing, and the Adam optimizer from torch.optim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FFCosJtpE82r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.optim import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT78qrwfdGHC"
      },
      "source": [
        "## Setting up the Device\n",
        "\n",
        "Next, the code checks if CUDA is available. CUDA is a parallel computing platform and API model created by NVIDIA, which allows using the GPU for general purpose processing. If CUDA is available, PyTorch will use the GPU for computations, otherwise, it will use the CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fi2ws1xNdCYD"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi-mHvdJGEq0",
        "outputId": "8a34197c-9b6f-4c3b-b9f1-bc5b9fe32694"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k50pI3OZdOsT"
      },
      "source": [
        "## Text Processing\n",
        "\n",
        "The code then loads a large corpus of text, converts it to lowercase, and tokenizes it using a basic English tokenizer from torchtext. Tokenization is the process of splitting the text into individual words or tokens. After tokenization, a vocabulary is built from the tokens, and the text is numericalized, i.e., each token is replaced by its index in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rXbc6cGCdLUE"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m big_text, abstract_list_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_abstracts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLLMs Generative AI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_paper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[2], line 9\u001b[0m, in \u001b[0;36mload_abstracts\u001b[1;34m(topic, number_paper)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_abstracts\u001b[39m(topic\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerative ai\u001b[39m\u001b[38;5;124m\"\u001b[39m, number_paper\u001b[38;5;241m=\u001b[39mMAX_PAPER):\n\u001b[0;32m      8\u001b[0m     sch \u001b[38;5;241m=\u001b[39m SemanticScholar()\n\u001b[1;32m----> 9\u001b[0m     papers \u001b[38;5;241m=\u001b[39m \u001b[43msch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_paper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2018-2023\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     big_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m     abstract_list \u001b[38;5;241m=\u001b[39m []\n",
            "File \u001b[1;32mc:\\Users\\panwe\\Desktop\\IE7374_Project2\\LLM\\Lib\\site-packages\\semanticscholar\\SemanticScholar.py:335\u001b[0m, in \u001b[0;36mSemanticScholar.search_paper\u001b[1;34m(self, query, year, publication_types, open_access_pdf, venue, fields_of_study, fields, publication_date_or_year, min_citation_count, limit, bulk, sort)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Search for papers by keyword. Performs a search query based on the \\\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m    S2 search relevance algorithm, or a bulk retrieval of basic paper \\\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m    data without search relevance (if bulk=True). Paper relevance \\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m:rtype: :class:`semanticscholar.PaginatedResults.PaginatedResults`\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    334\u001b[0m loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[1;32m--> 335\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_AsyncSemanticScholar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_paper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpublication_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpublication_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopen_access_pdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopen_access_pdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvenue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvenue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfields_of_study\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields_of_study\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpublication_date_or_year\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpublication_date_or_year\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_citation_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_citation_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbulk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbulk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
            "File \u001b[1;32mc:\\Users\\panwe\\Desktop\\IE7374_Project2\\LLM\\Lib\\site-packages\\nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\panwe\\Desktop\\IE7374_Project2\\LLM\\Lib\\site-packages\\nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m     heappop(scheduled)\n\u001b[0;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[0;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[0;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
            "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\selectors.py:323\u001b[0m, in \u001b[0;36mSelectSelector.select\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    321\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 323\u001b[0m     r, w, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
            "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\selectors.py:314\u001b[0m, in \u001b[0;36mSelectSelector._select\u001b[1;34m(self, r, w, _, timeout)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 314\u001b[0m     r, w, x \u001b[38;5;241m=\u001b[39m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w \u001b[38;5;241m+\u001b[39m x, []\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "big_text, abstract_list_data = load_abstracts(\"LLMs Generative AI\", number_paper=600)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"Generative AI, the most popular current approach to AI, consists of large language models (LLMs) that are trained to produce outputs that are plausible, but not necessarily correct. Although their abilities are often uncanny, they are lacking in aspects of reasoning, leading LLMs to be less than completely trustworthy. Furthermore, their results tend to be both unpredictable and uninterpretable. We lay out 16 desiderata for future AI, and discuss an alternative approach to AI which could theoretically address many of the limitations associated with current approaches: AI educated with curated pieces of explicit knowledge and rules of thumb, enabling an inference engine to automatically deduce the logical entailments of all that knowledge. Even long arguments produced this way can be both trustworthy and interpretable, since the full step-by-step line of reasoning is always available, and for each step the provenance of the knowledge used can be documented and audited. There is however a catch: if the logical language is expressive enough to fully represent the meaning of anything we can say in English, then the inference engine runs much too slowly. That's why symbolic AI systems typically settle for some fast but much less expressive logic, such as knowledge graphs. We describe how one AI system, Cyc, has developed ways to overcome that tradeoff and is able to reason in higher order logic in real time. We suggest that any trustworthy general AI will need to hybridize the approaches, the LLM approach and more formal approach, and lay out a path to realizing that dream.\",\n",
              " 'Requirements Engineering (RE) is a critical phase in software development including the elicitation, analysis, specification, and validation of software requirements. Despite the importance of RE, it remains a challenging process due to the complexities of communication, uncertainty in the early stages and inadequate automation support. In recent years, large-language models (LLMs) have shown significant promise in diverse domains, including natural language processing, code generation, and program understanding. This chapter explores the potential of LLMs in driving RE processes, aiming to improve the efficiency and accuracy of requirements-related tasks. We propose key directions and SWOT analysis for research and development in using LLMs for RE, focusing on the potential for requirements elicitation, analysis, specification, and validation. We further present the results from a preliminary evaluation, in this context.',\n",
              " 'A moderately detailed consideration of interactive LLMs as cognitive systems is given, focusing on LLMs circa mid-2023 such as ChatGPT, GPT-4, Bard, Llama, etc.. Cognitive strengths of these systems are reviewed, and then careful attention is paid to the substantial differences between the sort of cognitive system these LLMs are, and the sort of cognitive systems human beings are. It is found that many of the practical weaknesses of these AI systems can be tied specifically to lacks in the basic cognitive architectures according to which these systems are built. It is argued that incremental improvement of such LLMs is not a viable approach to working toward human-level AGI, in practical terms given realizable amounts of compute resources. This does not imply there is nothing to learn about human-level AGI from studying and experimenting with LLMs, nor that LLMs cannot form significant parts of human-level AGI architectures that also incorporate other ideas. Social and ethical matters regarding LLMs are very briefly touched from this perspective, which implies that while care should be taken regarding misinformation and other issues, and economic upheavals will need their own social remedies based on their unpredictable course as with any powerfully impactful technology, overall the sort of policy needed as regards modern LLMs is quite different than would be the case if a more credible approximation to human-level AGI were at hand.']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "abstract_list_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30wdgp3dneJK"
      },
      "source": [
        "Cache the data in a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AaWamGErmaww"
      },
      "outputs": [],
      "source": [
        "\n",
        "abstract_text = ' '.join(abstract_list_data)\n",
        "with open('genAIScolarData600.txt', 'w',encoding='utf-8') as output:\n",
        "    output.write(abstract_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9WXBVnRutqo"
      },
      "source": [
        "Perform Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "K4ietgkUIekA"
      },
      "outputs": [],
      "source": [
        "with open('genAIScolarData600.txt', 'r',encoding='utf-8') as input:\n",
        "    abstract_text = input.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xXcY7xHzmYKW"
      },
      "outputs": [],
      "source": [
        "# Lowercase the text\n",
        "text = abstract_text.lower()\n",
        "\n",
        "# Define the tokenizer\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_text = [list(tokenizer(text))]\n",
        "\n",
        "# Build the vocabulary from the tokenized text\n",
        "vocab = build_vocab_from_iterator(tokenized_text)\n",
        "\n",
        "# Numericalize the text\n",
        "numericalized_text = [vocab[token] for token in tokenized_text[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9482"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AILI7fOHdmKT"
      },
      "source": [
        "## Dataset Creation\n",
        "\n",
        "The code defines a custom PyTorch Dataset for the text data. In PyTorch, a Dataset is an abstract class representing a dataset, and it has two main methods: __len__ and __getitem__. The __len__ method returns the number of items in the dataset, and the __getitem__ method returns the item (a sequence of tokens) and its label (the next token in the sequence). The sequences are of a fixed length, defined by sequence_length.\n",
        "\n",
        "A DataLoader is then created for the dataset. The DataLoader is a PyTorch utility for loading data in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "fC6RKFXxdUTJ"
      },
      "outputs": [],
      "source": [
        "# Define the dataset\n",
        "class LlamaDataset(Dataset):\n",
        "    def __init__(self, text, sequence_length):\n",
        "        self.text = text\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.tensor(self.text[idx:idx+self.sequence_length]),\n",
        "            torch.tensor(self.text[idx+1:idx+self.sequence_length+1]),\n",
        "        )\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "sequence_length = 8\n",
        "dataset = LlamaDataset(numericalized_text, sequence_length)\n",
        "dataloader = DataLoader(dataset, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UmagdKyd4Tv"
      },
      "source": [
        "## Model Definition\n",
        "\n",
        "The code defines a custom PyTorch Module for the text generation model. The model consists of an embedding layer, a transformer layer, and a linear layer. The embedding layer converts the input tokens into vectors of a fixed size. The transformer layer is the main part of the model, and it learns the relationships between the words in the text. The linear layer converts the output of the transformer layer into predictions for the next word in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "t_D8EEnMdwOj"
      },
      "outputs": [],
      "source": [
        "class LlamaModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=embed_size,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=hidden_size,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.fc = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output = self.transformer(embedded, embedded)\n",
        "        output = self.fc(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEWN-0QSeA0Q"
      },
      "source": [
        "Simplified version of the model using GRU instead of Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PL5zlQQyeDrG"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "class LlamaModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        output = self.fc(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 8])\n",
            "torch.Size([128, 8])\n"
          ]
        }
      ],
      "source": [
        "for batch in dataloader:\n",
        "        x, y = batch\n",
        "        print(x.shape)\n",
        "        print(y.shape)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ749OQFeORj"
      },
      "source": [
        "## Model Initialization and Training\n",
        "\n",
        "The model is then initialized with the size of the vocabulary, the embedding size, the hidden size, the number of layers, the number of heads for the multi-head attention mechanism in the transformer, and the dropout rate. The model is moved to the GPU if available.\n",
        "\n",
        "If multiple GPUs are available, the model is wrapped with nn.DataParallel, which allows parallelizing the computations over the GPUs.\n",
        "\n",
        "The Adam optimizer is initialized with the model parameters and a learning rate of 0.001.\n",
        "\n",
        "The model is then trained for 80 epochs. In each epoch, the model goes through all the data in the dataloader. For each batch, the model makes predictions for the next word in the sequence, computes the cross-entropy loss between the predictions and the actual next words, and updates the model parameters to minimize the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhI_V44FeQ8r",
        "outputId": "ac66ded7-0e77-46d1-b449-7ff8ed422b54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss 5.322413921356201\n",
            "Epoch 1, Loss 3.9353713989257812\n",
            "Epoch 2, Loss 2.745954990386963\n",
            "Epoch 3, Loss 1.8482836484909058\n",
            "Epoch 4, Loss 1.3274807929992676\n",
            "Epoch 5, Loss 1.0970139503479004\n",
            "Epoch 6, Loss 0.944744884967804\n",
            "Epoch 7, Loss 0.7786787748336792\n",
            "Epoch 8, Loss 0.7251054048538208\n",
            "Epoch 9, Loss 0.6376631855964661\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model and the optimizer\n",
        "model = LlamaModel(len(vocab), embed_size=128, hidden_size=256, num_layers=2, num_heads=8, dropout=0.1).to(device)\n",
        "\n",
        "# If there are multiple GPUs, wrap the model with nn.DataParallel\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(10):\n",
        "    for batch in dataloader:\n",
        "        x, y = batch\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x)\n",
        "        loss = nn.functional.cross_entropy(y_pred.view(-1, len(vocab)), y.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch}, Loss {loss.item()}')\n",
        "    if float(loss.item()) < 0.06:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPqWFnmaeYbf"
      },
      "source": [
        "# Result\n",
        "## Text Generation\n",
        "Finally, the trained model is used to generate new text. A seed text is provided as a starting point, and the model generates a specified number of tokens following the seed text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "aOaK5dpJee2x"
      },
      "outputs": [],
      "source": [
        "# Use the trained model to generate new text\n",
        "def generate_text(model, human_input, num_tokens):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # No need to track the gradients\n",
        "        tokens = [vocab[token] for token in tokenizer(human_input)]\n",
        "        tokens = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "        for _ in range(num_tokens):\n",
        "            output = model(tokens)\n",
        "            probabilities = nn.functional.softmax(output[0, -1], dim=0)\n",
        "            next_token = torch.multinomial(probabilities, 1).item()\n",
        "            tokens = torch.cat([tokens, torch.tensor([[next_token]]).to(device)], dim=1)\n",
        "        generated_text = ' '.join(vocab.get_itos()[token] for token in tokens[0].cpu().numpy())\n",
        "        return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hho7j3Yeic2"
      },
      "source": [
        "Example 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxjBpLaFehwe",
        "outputId": "65b34be1-6d59-4464-e517-520dbf6a3136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generative ai is generative ai is generative ai is generative ai is generative ai is generative ai is generative ai is is generative ai is part is generative ai is generative ai is generative ai is generative ai is generative ai is is generative ai is generative ai is released generative ai is generative ai is open generative ai is is released generative ai is generative ai is is generative ai is released generative ai is generative ai is generative ai is generative ai is is released generative ai is released generative ai is released is released generative ai is released generative ai\n"
          ]
        }
      ],
      "source": [
        "result = generate_text(model, human_input=\"Generative AI is \", num_tokens=100)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BImDQ28JeoG8"
      },
      "source": [
        "Example 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzTv77WgencR",
        "outputId": "ccc5a8d2-e2d3-409c-c655-c5e6f821bbbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "intelligence is ai is is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that is that\n"
          ]
        }
      ],
      "source": [
        "result = generate_text(model, human_input=\"Intelligence is \", num_tokens=100)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbcal20Ye1MR"
      },
      "source": [
        "Example3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEKnZtztervE",
        "outputId": "a4f3c21c-122b-4d01-b163-8b055481009c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question answering system can can speed to to to to to the the the the the the the the the the the it detection that that that that from from from text-based the the the the the to only had in in found dt users innovation months the the the the to only context engagement in prompt interest a similar generative generative generative design drivers enabling the in only concerns interesting communities action generative generative network ( ) ) ) ) ) ) ) ( ) ) ( ) ) ( ) ( ) , , , , , , , complexity . essays\n"
          ]
        }
      ],
      "source": [
        "result = generate_text(model, human_input=\"Question answering system can \", num_tokens=100)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3-8pDv_fKfG"
      },
      "source": [
        "## Model summary and overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4rM6uh0fOcz",
        "outputId": "2091cc68-3c92-4462-fcb1-e1f597a4905a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 3,099,914 trainable parameters\n",
            "The model has 9482 tokens\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "print(f'The model has {len(vocab)} tokens')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abMWxf2yAwTw",
        "outputId": "c28d8bd4-259a-4e2b-cb92-ca6f56a55df0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: torch in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (from torchviz) (2.2.2+cu121)\n",
            "Collecting graphviz (from torchviz)\n",
            "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (from torch->torchviz) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (from torch->torchviz) (4.8.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (from torch->torchviz) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (from torch->torchviz) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (from torch->torchviz) (2023.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (from jinja2->torch->torchviz) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\panwe\\desktop\\ie7374_project2\\llm\\lib\\site-packages (from sympy->torch->torchviz) (1.3.0)\n",
            "Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
            "   ---------------------------------------- 0.0/47.1 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/47.1 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/47.1 kB ? eta -:--:--\n",
            "   ---------------------------------- ----- 41.0/47.1 kB 991.0 kB/s eta 0:00:01\n",
            "   ---------------------------------------- 47.1/47.1 kB 801.9 kB/s eta 0:00:00\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (pyproject.toml): started\n",
            "  Building wheel for torchviz (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4139 sha256=a2d085bf7231e8ed296bcf4257c844aba18efa2df76ddb902d9bc563216783e6\n",
            "  Stored in directory: c:\\users\\panwe\\appdata\\local\\pip\\cache\\wheels\\98\\f2\\3d\\290537e0ff7f67aaa647847a10fb5ee5eca305b3c41a774523\n",
            "Successfully built torchviz\n",
            "Installing collected packages: graphviz, torchviz\n",
            "Successfully installed graphviz-0.20.3 torchviz-0.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "uMBkP52TfPLg"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'distutils'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# visualize the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchviz\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Variable\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create a variable with the size of your input\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\panwe\\Desktop\\IE7374_Project2\\LLM\\Lib\\site-packages\\torchviz\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot, make_dot_from_trace\n",
            "File \u001b[1;32mc:\\Users\\panwe\\Desktop\\IE7374_Project2\\LLM\\Lib\\site-packages\\torchviz\\dot.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m namedtuple\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Digraph\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
          ]
        }
      ],
      "source": [
        "# visualize the model\n",
        "import torchviz\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Create a variable with the size of your input\n",
        "x = torch.randint(high=len(vocab), size=(1, 30), dtype=torch.long).to(device)\n",
        "\n",
        "# Generate a diagram for a specific model\n",
        "y = model(x)\n",
        "torchviz.make_dot(y.mean(), params=dict(model.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "03uwBr4aBDGG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 30])\n",
            "torch.Size([128, 30])\n"
          ]
        }
      ],
      "source": [
        "for batch in dataloader:\n",
        "        x, y = batch\n",
        "        print(x.shape)\n",
        "        print(y.shape)\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
